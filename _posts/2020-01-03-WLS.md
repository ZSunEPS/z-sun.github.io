---
layout: distill
title: Weighted Least Squares (WLS)
description: Handy notes on the weighted least squres linear regression
tags: math regression
giscus_comments: false
date: 2020-01-03
featured: false

authors:
  - name: Zeyang Sun
    url: ""
    affiliations:
      name: Geology & Geophysics, TAMU

#bibliography: 2018-12-22-distill.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Introduction
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: Formulation
  - name: Codes in Julia
  - name: 
  - name: 
  - name: 
  - name: 

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }

---

## Introduction
Weighted least squares (WLS), also known as weighted linear regression, is a generalization of ordinary least squares (OLS) and linear regression in which knowledge of the unequal variance of observations (heteroscedasticity) is incorporated into the regression. WLS is also a specialization of generalized least squares, when all the off-diagonal entries of the covariance matrix of the errors are null. The model under consideration is 

$$
\mathbf{Y} = \mathbf{A}\boldsymbol{\beta} + \epsilon^{*}
$$

$$
\begin{pmatrix}
   y_{1}\\
   y_{2}\\
   \vdots\\
   y_{n}
\end{pmatrix} = \begin{pmatrix}
   1 & x_{1}  \\
   1 & x_{2}  \\
   \vdots & \vdots  \\
   1 & x_{n} 
\end{pmatrix} \begin{pmatrix}
   \beta_{0}\\
   \beta_{1}
\end{pmatrix}+\epsilon^{*}
$$

where $$ \epsilon^{*} $$ is assumed to be (multivariate) normally distributed with mean vector $$\mathbf{0}$$ and nonconstant variance-covariance matrix $$\boldsymbol{\Omega}$$. This assumption is expressed as 

$$
\text{E}(\epsilon^{*})=\mathbf{0}, \qquad
\text{Var}(\epsilon^{*})=\boldsymbol{\Omega}=\begin{pmatrix}
   \sigma_{1}^{2} & 0  & \dots  & 0\\ 
   0 & \sigma_{2}^{2} & \dots & 0\\ 
   \vdots & \vdots & \ddots & \vdots\\ 
   0 & 0 & \dots & \sigma_{n}^{2}
 \end{pmatrix}
$$


We define the reciprocal of each variance, $$ \sigma_{i}^{2} $$, as the weight, $$ w_{i}=1/\sigma_{i}^{2} $$, then let matrix $$ \mathbf{W} $$ be a diagonal matrix containing these weights:

$$
\mathbf{W} = \begin{pmatrix}
   w_{1} & 0  & \dots  & 0\\ 
   0 & w_{2} & \dots & 0\\ 
   \vdots & \vdots & \ddots & \vdots\\ 
   0 & 0 & \dots & w_{n}
 \end{pmatrix} = \begin{pmatrix}
   1/\sigma_{1}^{2} & 0  & \dots  & 0\\ 
   0 & 1/\sigma_{2}^{2} & \dots & 0\\ 
   \vdots & \vdots & \ddots & \vdots\\ 
   0 & 0 & \dots & 1/\sigma_{n}^{2}
 \end{pmatrix}
$$

The weighted least squares estimate is then

$$
\hat{\beta}_{WLS}={\underset {\boldsymbol {\beta }}{\operatorname {arg\ min} }}\,\sum _{i=1}^{n}\epsilon_{i}^{*2}=(\mathbf{A^{\textsf{T}}WA})^{-1}\mathbf{A^{\textsf{T}}WY}
$$

## Formulation

The fit of a model to a data point is measured by its residual, $$r_{i}$$, defined as the difference between a measured value of the dependent variable, $$y_{i}$$ and the value predicted by the model, $$f(x_{i},{\boldsymbol {\beta }})$$:

$$
r_{i}(\boldsymbol {\beta}) = y_{i}-f(x_{i}, \boldsymbol{\beta})
$$

If the errors are uncorrelated and have equal variance, then the (cost) function

$$
SE(\boldsymbol{\beta}) = \sum _{i}r_{i}^{2}(\boldsymbol{\beta})
$$

is minimized at $$\hat{\boldsymbol{\beta}}$$, such that $$\frac{\partial SE}{\partial \beta_{j}}(\hat{\boldsymbol{\beta}})=0$$. Aitken showed that when a weighted sum of squared residuals is minimized, $${\hat {\boldsymbol {\beta }}}$$ is the best linear unbiased estimator (BLUE) if each weight is equal to the reciprocal of the variance of the measurement

$$
SE = \sum _{i=1}^{n}W_{i}r_{i}^{2},\qquad W_{i} = \frac{1}{\sigma_{i}^{2}}
$$

Let the first derivate of this sum of squares equal to zero

$$
\sum _{i=1}^{n}\sum _{k=1}^{m}W_{i}\frac{\partial f(x_{i}, \boldsymbol{\beta})}{\partial \beta_{j}}r_{i}=0,\qquad k = 1, 2, ...,m
$$

which, in a linear least squares system give the modified normal equations

$$
\sum _{i=1}^{n}A_{i}W_{i}y_{i}=\sum _{i=1}^{n}\sum _{k=1}^{m}A_{i}W_{i}A_{i}\hat{\beta}_{k}
$$

When the observational errors (y errors) are uncorrelated and the weight matrix, $$\mathbf{W}$$, is diagonal, these may be written as

$$
\mathbf{A^{\textsf{T}}WY} = (\mathbf{A^{\textsf{T}}WA})\boldsymbol{\beta}
$$

Thus, the solution of $$\boldsymbol{\beta}$$ is

$$
\hat{\beta}_{WLS}=(\mathbf{A^{\textsf{T}}WA})^{-1}\mathbf{A^{\textsf{T}}WY}
$$

If we use the matrix form, we can write the sum of squared residuals as 

$$
SE(\boldsymbol{\beta})=\sum_{i=1}^{n}w_{i}(Y_{i}-\mathbf{A}_{i}\boldsymbol{\beta})^{2}
$$

where $$\mathbf{A}_{i}$$ is the $$i^{th}$$ row of $$\mathbf{A}$$. Because we write $$\mathbf{W}$$ for the matrix with the $$w_{i}$$ on the diagonal and zeros everywhere else, then

$$
\begin{aligned}
SE(\boldsymbol{\beta}) & = (\mathbf{Y-A\boldsymbol{\beta}})^{\textsf{T}}\mathbf{W}(\mathbf{Y-A\boldsymbol{\beta}})\\
& = \mathbf{Y}^{\textsf{T}}\mathbf{WY}-\mathbf{Y}^{\textsf{T}}\mathbf{WA\boldsymbol{\beta}}-\boldsymbol{\beta}^{\textsf{T}}\mathbf{A}^{\textsf{T}}\mathbf{WY} + \boldsymbol{\beta}^{\textsf{T}}\mathbf{A}^{\textsf{T}}\mathbf{WA\boldsymbol{\beta}}
\end{aligned}
$$

Differentiating with respect to $$\boldsymbol{\beta}$$, we get the gradient as

$$
\nabla _{\boldsymbol{\beta}}SE = 2(-\mathbf{A}^{\textsf{T}}\mathbf{WY}+\mathbf{A}^{\textsf{T}}\mathbf{WA}\boldsymbol{\beta})
$$

Setting this to zero at the optimum and solving,

$$
\hat{\beta}_{WLS}=(\mathbf{A^{\textsf{T}}WA})^{-1}\mathbf{A^{\textsf{T}}WY}
$$

## Codes in Julia

We can write several lines of Julia codes to solve $$\boldsymbol{\beta}$$ for a weighted least squares linear regression with the form $$y=\beta_{1}x+\beta_{0}$$

{% highlight julia %}
  using LinearAlgebra, Random
  rng1 = MersenneTwister(1234)
  x = [1.0, 3.0, 7.0, 13.0, 19.0, 24.0]
  y = 2.0 .* x .+ 3.0 .+ randn(rng1, 6)
  rng2 = MersenneTwister(4321)
  σ = abs.(randn(rng2, 6))
  W = Diagonal(1 ./ σ.^2)
  A = [[1.0 for i in x] x]
  β = inv(X' * W * X) * X' * W * y # β = [β₀, β₁], where β₀ is intercept and β₁ is the slope.
{% endhighlight %}

{% highlight julia %}
2-element Vector{Float64}:
 3.261206161053625
 2.001618372891876
{% endhighlight %}

In this simple case, `x` is the independent variable (e.g., temperature, pressure, etc.) whose uncertainty is ignored here. `y` is the dependent variable or observation, and its uncertainty is the measurement uncertainty (e.g., one standard error, 1SE, of multiple replicates of one sample).



